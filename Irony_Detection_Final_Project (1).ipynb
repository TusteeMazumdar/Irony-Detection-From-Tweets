{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vlra-8Hm7H8N",
        "outputId": "6129dcf4-8d2d-4afe-bd6c-38c475b81f91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993221 sha256=eef4e011230c409077fffb70315f05c0f678abf3c22ffe4918342bb5adb94572\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ],
      "source": [
        "!pip install langdetect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9x14VcNhXPvO",
        "outputId": "e7cbf4be-f350-46b7-dce6-3e09034daff2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting krovetzstemmer\n",
            "  Downloading KrovetzStemmer-0.8.tar.gz (112 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/112.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.9/112.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: krovetzstemmer\n",
            "  Building wheel for krovetzstemmer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for krovetzstemmer: filename=KrovetzStemmer-0.8-cp310-cp310-linux_x86_64.whl size=373245 sha256=08c54a99f3d6d1fb30932da79e8cd691e218aa77d7d65af6abe65d8345004566\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/82/b0/1677fc180ec3a9eaa9902ea38650bca7e0211d052a3bd8ce39\n",
            "Successfully built krovetzstemmer\n",
            "Installing collected packages: krovetzstemmer\n",
            "Successfully installed krovetzstemmer-0.8\n"
          ]
        }
      ],
      "source": [
        " !pip install krovetzstemmer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langdetect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcc7xvvJYAQJ",
        "outputId": "9689419e-7168-485b-ed76-f6df7db220fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993221 sha256=704367c597c3ff2acfd05fc777bd3626876e644e52cd33462966ed8c14469272\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install emoticon_fix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJ09sIkJYK7d",
        "outputId": "cc7c4bc0-f735-454b-92c4-6dded30f39e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoticon_fix\n",
            "  Downloading emoticon_fix-0.0.2.tar.gz (3.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: emoticon_fix\n",
            "  Building wheel for emoticon_fix (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoticon_fix: filename=emoticon_fix-0.0.2-py3-none-any.whl size=3428 sha256=fbdac581d08781ddcd5700fbcda545e6a996490fc035d93d30f89c6c288879a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/e4/98/2d97814297a56155deb62f83bb59cbad836f0a7f2d59671b86\n",
            "Successfully built emoticon_fix\n",
            "Installing collected packages: emoticon_fix\n",
            "Successfully installed emoticon_fix-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYuadqYo1UZv",
        "outputId": "fc4d871e-abc5-4bea-806f-d481052601f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emot\n",
            "  Downloading emot-3.1-py3-none-any.whl.metadata (396 bytes)\n",
            "Downloading emot-3.1-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emot\n",
            "Successfully installed emot-3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install emoticon_fix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQTiJQuuBJUe",
        "outputId": "d04b40f9-43c3-4365-ea52-a25424b95493"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoticon_fix\n",
            "  Downloading emoticon_fix-0.0.2.tar.gz (3.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: emoticon_fix\n",
            "  Building wheel for emoticon_fix (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoticon_fix: filename=emoticon_fix-0.0.2-py3-none-any.whl size=3428 sha256=a4beb14be8ca74e5350a1b7e76947e1515fd83b77c7fa07fb46da524bc037801\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/e4/98/2d97814297a56155deb62f83bb59cbad836f0a7f2d59671b86\n",
            "Successfully built emoticon_fix\n",
            "Installing collected packages: emoticon_fix\n",
            "Successfully installed emoticon_fix-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect\n",
        "!pip install emoticon_fix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xaP5tj9KUcp",
        "outputId": "7ebb0342-b3e3-4c05-8929-c7d65de0269f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993221 sha256=a9e0ffdb552401aac07638c264977d0c87f2a5190e27ff26038c0af52cc55ff7\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n",
            "Collecting emoticon_fix\n",
            "  Downloading emoticon_fix-0.0.2.tar.gz (3.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: emoticon_fix\n",
            "  Building wheel for emoticon_fix (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoticon_fix: filename=emoticon_fix-0.0.2-py3-none-any.whl size=3428 sha256=5d940917151052984e0bf49f790f972f0767b2248f027efb269e7bcc7f17e58b\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/e4/98/2d97814297a56155deb62f83bb59cbad836f0a7f2d59671b86\n",
            "Successfully built emoticon_fix\n",
            "Installing collected packages: emoticon_fix\n",
            "Successfully installed emoticon_fix-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect\n",
        "!pip install emoticon_fix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzDr2bqZ95u7",
        "outputId": "ffd08669-2188-432e-a1c2-b520a67bebfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993221 sha256=2ae5f08a66c6b6d2b1c45cebf253cb95f7acaa7fc236c69024e73e7bd8c99239\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n",
            "Collecting emoticon_fix\n",
            "  Downloading emoticon_fix-0.0.2.tar.gz (3.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: emoticon_fix\n",
            "  Building wheel for emoticon_fix (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoticon_fix: filename=emoticon_fix-0.0.2-py3-none-any.whl size=3428 sha256=746e28e4cc270db0cf0ce9b0a30de72da4a7ccbdbc6280c0d2f4efc26390fb5c\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/e4/98/2d97814297a56155deb62f83bb59cbad836f0a7f2d59671b86\n",
            "Successfully built emoticon_fix\n",
            "Installing collected packages: emoticon_fix\n",
            "Successfully installed emoticon_fix-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import csv\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize  # For tokenization purpose\n",
        "from nltk.corpus import stopwords  # For stopword purpose\n",
        "from langdetect import detect  # For language detection purpose\n",
        "import unicodedata  # For accented character purpose\n",
        "from nltk.stem.snowball import SnowballStemmer  # For stemming purpose\n",
        "from emoticon_fix import emoticon_fix  # For Emoticon Purpose\n",
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np  # For array handling\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.metrics import confusion_matrix, recall_score, precision_score, f1_score, accuracy_score, classification_report\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))  # Load stopwords\n",
        "\n",
        "\"\"\"Lexical Normalization purpose\"\"\"\n",
        "# declaring Global dictionary for lexical normalization\n",
        "lexical_normalization_dict = {}\n",
        "\n",
        "# Read Lexical Normalizion data file\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/LexicalNormalizationData.txt', 'r') as f:\n",
        "    next(f)  # Skip headings\n",
        "    reader = csv.reader(f, dialect=\"excel-tab\")\n",
        "    for line in reader:\n",
        "        lexical_normalization_dict[line[0]] = line[1]\n",
        "# Data Preprocessing Steps(6):\n",
        "\n",
        "# Step1: Convert Emoticons into Words:\n",
        "def Emoticon_Conversion_Text(text):\n",
        "    return emoticon_fix.emoticon_fix(text)\n",
        "\n",
        "# Step2: Tokenization:\n",
        "def performTokenization(text):  # Tokenize text\n",
        "    Tokenized_words = word_tokenize(text)  # Tokenize the input text to individual words\n",
        "    return Tokenized_words  # Return the tokenized words\n",
        "\n",
        "# Step3: Language Identification:\n",
        "def performLanguageIdentification(text):\n",
        "    return detect(text)\n",
        "\n",
        "# Step 4: Accented Character:\n",
        "def performAccentedCharacter(text):\n",
        "    # Normalize the text to decompose characters into base characters\n",
        "    normalized_text = unicodedata.normalize('NFD', text)\n",
        "    # Encode the text to ASCII bytes, ignoring non-ASCII characters\n",
        "    ascii_bytes = normalized_text.encode('ascii', 'ignore')\n",
        "    # Decode the ASCII bytes back to a string\n",
        "    WithoutAccentedCharacter_text = ascii_bytes.decode('utf-8')\n",
        "    return WithoutAccentedCharacter_text\n",
        "\n",
        "# Step5: Stemming:\n",
        "def performStemming(Tokenized_words):\n",
        "    stemmer = SnowballStemmer(\"english\")\n",
        "    stemmed_words = [stemmer.stem(word) for word in Tokenized_words]\n",
        "    return stemmed_words\n",
        "\n",
        "# Step6: Removing STOPWORDS:\n",
        "def performStopWordRemoval(stemmed_words):\n",
        "    tokens_without_sw = [word for word in stemmed_words if word not in stop_words]\n",
        "    return tokens_without_sw\n",
        "\n",
        "#Step-7: Hashtag Segmentation:\n",
        "def performHashtagSegmentation(text):\n",
        "    words = text.split()  # Split text into words\n",
        "    words_without_hashtags = [word[1:] if word.startswith('#') else word for word in words]  # Remove '#' from hashtags\n",
        "    return ' '.join(words_without_hashtags)\n",
        "\n",
        "#Step-8: Lexical Normalization:\n",
        "def performLexicalNormalization(text):\n",
        "\n",
        "    words = text.split()  # Split text into words\n",
        "    normalized_words = [lexical_normalization_dict.get(word, word) for word in words]  # Replace words if they exist in the dictionary\n",
        "    normalized_text = ' '.join(normalized_words)  # Join the words back into a single string\n",
        "    return normalized_text\n",
        "\n",
        "\n",
        "\n",
        "# Function for preprocessing module\n",
        "def preProcessingModule(text):\n",
        "    without_accented = performAccentedCharacter(text)  # Remove accents from the text\n",
        "    text_with_emojis = Emoticon_Conversion_Text(without_accented)  # Convert emoticons to words\n",
        "\n",
        "    text_with_lexical_normalization = performLexicalNormalization(text_with_emojis)\n",
        "\n",
        "    text_without_hashtags = performHashtagSegmentation(text_with_lexical_normalization)  # Remove hashtags while keeping words intact\n",
        "\n",
        "    tokenized_words = performTokenization(text_without_hashtags)  # Tokenize the text into individual words\n",
        "    stemmed_words = performStemming(tokenized_words)  # Stem each tokenized word to its base form\n",
        "    Without_Stopwords = performStopWordRemoval(stemmed_words)  # Remove stopwords from the text\n",
        "    Finalprocessed_text = ' '.join(Without_Stopwords)\n",
        "    language = performLanguageIdentification(text)  # Perform language identification\n",
        "    #print(\"Detected language:\", language)\n",
        "    return Finalprocessed_text\n",
        "\n",
        "\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    tweets = []\n",
        "    label = []\n",
        "    csv.field_size_limit(500 * 1024 * 1024)\n",
        "    with open('/content/drive/MyDrive/Colab Notebooks/IronyDetectionSmallVersionDataset.txt', 'r') as f:\n",
        "        next(f)  # Skip headings\n",
        "        reader = csv.reader(f, dialect=\"excel-tab\")\n",
        "        for line in reader:\n",
        "            print(line[2])\n",
        "            preProcessedTweetText = preProcessingModule(line[2])  # Preprocess text\n",
        "            print(preProcessedTweetText)\n",
        "            tweets.append(preProcessedTweetText)\n",
        "            if line[1] == \"1\":\n",
        "                label.append(\"Irony\")\n",
        "            else:\n",
        "                label.append(\"Non-Irony\")\n",
        "\n",
        "    # Converting list to array in python:\n",
        "    X_train = np.array(tweets)\n",
        "    Y_train = np.array(label)\n",
        "    print()\n",
        "    print(X_train)\n",
        "    print()\n",
        "    print(Y_train)\n",
        "    print()\n",
        "\n",
        "   #Applying 10 different Classifiers:\n",
        "\n",
        "    # Classifier-1: SGD classifier:\n",
        "    from sklearn.linear_model import SGDClassifier\n",
        "    #Classifier-2: NB classifier:\n",
        "    from sklearn.naive_bayes import MultinomialNB\n",
        "    #Classifier-3: Linear SVC classifier:\n",
        "    from sklearn.svm import SVC\n",
        "    from sklearn.svm import LinearSVC\n",
        "    #Classifier-4: Logistic Regression classifier:\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    #Classifier-5: MLP Classifier\n",
        "    from sklearn.neural_network import MLPClassifier\n",
        "    #Classifier-6: KNN Classifier\n",
        "    from sklearn.neighbors import KNeighborsClassifier\n",
        "    #Classifier-7: Decision Tree Classifier\n",
        "    from sklearn.tree import DecisionTreeClassifier\n",
        "    #Classifier-8: Random Forest Classifier\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    #Classifier-9: AdaBoost Classifier\n",
        "    from sklearn.ensemble import AdaBoostClassifier\n",
        "    #Classifier-10: Gradient Boosting Classifier\n",
        "    from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "\n",
        "    # Define Classifier or Extracting Features:\n",
        "    classifier = Pipeline([  # Pipeline- all 3 segments are combined into a single module\n",
        "        ('count_vectorizer', CountVectorizer(ngram_range=(1, 3))),  # CountVectorizer- Convert document into word matrix using only TF\n",
        "        ('tfidf', TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True)),  # Using IDF; smooth_IDF formula to eliminate division by zero error\n",
        "        ('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=0.000000001)) #F1 Score = 0.625(HIGHEST)\n",
        "        # ('clf',MultinomialNB(alpha=0.001, fit_prior=True, class_prior=None)) #f1 score = 0.525\n",
        "        #('clf',LinearSVC(penalty='l2', loss='squared_hinge', tol=0.0001, C=1.0)) #f1 score= 0.35\n",
        "        #('clf',LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='liblinear', max_iter=100, multi_class='ovr', verbose=0)) #F1 score = 0.35\n",
        "        #('clf', MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam', alpha=0.000000001)) #F1 score = 0.55\n",
        "        #('clf', KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)) #F1 score = 0.40\n",
        "        #('clf', DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, min_samples_split=2)) #F1 score = 0.55\n",
        "        #('clf', RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=None)) #F1 score = 0.35\n",
        "        #('clf', AdaBoostClassifier(n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None)) #F1 score = 0.45\n",
        "         #('clf', GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0)) #F1 score = 0.325\n",
        "    ])\n",
        "    \"\"\"\n",
        "    L2 norm - ensures that the sum of the squares of the vector elements equals 1\n",
        "    \"\"\"\n",
        "\n",
        "    # Train (fit) Classifier\n",
        "    \"\"\"\n",
        "    fit- runs through each step in the Pipeline sequentially, applying transformations\n",
        "    and finally fitting the classifier\n",
        "    \"\"\"\n",
        "    classifier.fit(X_train, Y_train)\n",
        "\n",
        "\n",
        "    #Test dataset\n",
        "    testTweets = []\n",
        "    testLabelGold = []  # True labels\n",
        "    csv.field_size_limit(500 * 1024 * 1024)\n",
        "    with open('/content/drive/MyDrive/Colab Notebooks/IronyDetectionSmall_TestDataset.txt', 'r') as f:\n",
        "        next(f)\n",
        "        reader = csv.reader(f, dialect=\"excel-tab\")\n",
        "        for line in reader:\n",
        "            print(line[2])\n",
        "            preProcessedTweetText = preProcessingModule(line[2])\n",
        "            print(preProcessedTweetText)\n",
        "            testTweets.append(preProcessedTweetText)\n",
        "            if line[1] == \"1\":\n",
        "                testLabelGold.append(\"Irony\")\n",
        "            else:\n",
        "                testLabelGold.append(\"Non-Irony\")\n",
        "\n",
        "    # test_label_prediction\n",
        "    X_test = np.array(testTweets)\n",
        "    testLabelPredicted = classifier.predict(X_test)  # Predicted labels\n",
        "\n",
        "    \"\"\"\n",
        "    Classifier- model that has been trained to recognize patterns and make predictions\n",
        "    predict- method of classifier that takes input data and outputs the predicted labels\n",
        "    \"\"\"\n",
        "\n",
        "    # Evaluation (2D Matrix)\n",
        "    results = confusion_matrix(testLabelGold, testLabelPredicted)\n",
        "\n",
        "    print('Confusion Matrix: ')\n",
        "    print(results)\n",
        "\n",
        "    print('Recall Score: ', recall_score(testLabelGold, testLabelPredicted, average='micro'))\n",
        "    print('Precision Score: ', precision_score(testLabelGold, testLabelPredicted, average=None))\n",
        "    print('Micro-Average F1 Score: ', f1_score(testLabelGold, testLabelPredicted, average='micro'))\n",
        "    print('Accuracy: ', accuracy_score(testLabelGold, testLabelPredicted))\n",
        "\n",
        "    print('Evaluation Report: ')\n",
        "    print(classification_report(testLabelGold, testLabelPredicted))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DbUNOAAng0RW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a37c3f97-be1b-4bd7-b143-c78e37504007"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sweet United Nations video. Just in time for Christmas. #imagine #NoReligion  http://t.co/fej2v3OUBR\n",
            "sweet unit nation video . time christma . imagin noreligion http : //t.co/fej2v3oubr\n",
            "@mrdahl87 We are rumored to have talked to Erv's agent... and the Angels asked about Ed Escobar... that's hardly nothing    ;)\n",
            "@ mrdahl87 rumor talk erv 's agent ... angel ask ed escobar ... 's hard noth smirk\n",
            "Hey there! Nice to see you Minnesota/ND Winter Weather \n",
            "hey ! nice see minnesota / nd winter weather\n",
            "3 episodes left I'm dying over here\n",
            "3 episod left 'm die\n",
            "I can't breathe! was chosen as the most notable quote of the year in an annual list released by a Yale University librarian \n",
            "ca n't breath ! chosen notabl quot year annual list releas yale univers librarian\n",
            "You're never too old for Footie Pajamas. http://t.co/ElzGqsX2yQ\n",
            "never old footi pajama . http : //t.co/elzgqsx2yq\n",
            "Nothing makes me happier then getting on the highway and seeing break lights light up like a Christmas tree.. \n",
            "noth make happier get highway see break light light like christma tree ..\n",
            "4:30 an opening my first beer now gonna be a long night/day\n",
            "4:30 open first beer gon na long night / day\n",
            "@Adam_Klug do you think you would support a guy who knocked out your daughter? Rice doesn't deserve support.\n",
            "@ adam_klug think would support guy knock daughter ? rice doe n't deserv support .\n",
            "@samcguigan544 You are not allowed to open that until Christmas day!\n",
            "@ samcguigan544 allow open christma day !\n",
            "Oh, thank GOD - our entire office email system is down... the day of a big event. Santa, you know JUST what to get me for xmas. \n",
            "oh , thank god - entir offic email system ... day big event . santa , know get christma .\n",
            "But instead, I'm scrolling through Facebook, Instagram, and Twitter for hours on end, accomplishing nothing. \n",
            "instead , 'm scroll facebook , instagram , twitter hour end , accomplish noth .\n",
            "@TargetZonePT :pouting_face: no he bloody isn't I was upstairs getting changed !\n",
            "@ targetzonept cheeki outing_fac : bloodi n't upstair get chang !\n",
            "Cold or warmth both suffuse one's cheeks with pink (colour/tone) ... Do you understand the underlying difference & its texture?\n",
            "cold warmth suffus one 's cheek pink ( color / tone ) ... understand differ & textur ?\n",
            "Just great when you're mobile bill arrives by text \n",
            "great mobil bill arriv text\n",
            "crushes are great until you realize they'll never be interested in you.| :p\n",
            "crush great realiz never interest . | cheeki\n",
            "Buffalo sports media is smarter than all of us. Where else can you get the quality insight offered by Harrington and Busgaglia. \n",
            "buffalo sport media smarter us . els get qualiti insight offer harrington busgaglia .\n",
            "I guess my cat also lost 3 pounds when she went to the vet after I've been feeding her a few times a day.||#Eating|#food| |#WorkingOut\n",
            "guess cat also lost 3 pound went vet feed time day . | | eat | food | | workingout\n",
            "@YankeesWFAN @Ken_Rosenthal trading a SP for a defense-only SS? Brilliant trade. \n",
            "@ yankeeswfan @ ken_rosenth trade sp defense-on ss ? brilliant trade .\n",
            "But @DarklightDave was trying to find us, and my battery died. Guess how he found us? Yes, that bastard wand! !!!!!\n",
            "@ darklightdav tri find us , batteri die . guess found us ? yes , bastard wand ! ! ! !\n",
            "@deputymartinski please do..i need the second hand embarrassment so desperatly on my phone \n",
            "@ deputymartinski pleas .. need second hand embarrass desper phone\n",
            "\n",
            "['sweet unit nation video . time christma . imagin noreligion http : //t.co/fej2v3oubr'\n",
            " \"@ mrdahl87 rumor talk erv 's agent ... angel ask ed escobar ... 's hard noth smirk\"\n",
            " 'hey ! nice see minnesota / nd winter weather' \"3 episod left 'm die\"\n",
            " \"ca n't breath ! chosen notabl quot year annual list releas yale univers librarian\"\n",
            " 'never old footi pajama . http : //t.co/elzgqsx2yq'\n",
            " 'noth make happier get highway see break light light like christma tree ..'\n",
            " '4:30 open first beer gon na long night / day'\n",
            " \"@ adam_klug think would support guy knock daughter ? rice doe n't deserv support .\"\n",
            " '@ samcguigan544 allow open christma day !'\n",
            " 'oh , thank god - entir offic email system ... day big event . santa , know get christma .'\n",
            " \"instead , 'm scroll facebook , instagram , twitter hour end , accomplish noth .\"\n",
            " \"@ targetzonept cheeki outing_fac : bloodi n't upstair get chang !\"\n",
            " \"cold warmth suffus one 's cheek pink ( color / tone ) ... understand differ & textur ?\"\n",
            " 'great mobil bill arriv text'\n",
            " 'crush great realiz never interest . | cheeki'\n",
            " 'buffalo sport media smarter us . els get qualiti insight offer harrington busgaglia .'\n",
            " 'guess cat also lost 3 pound went vet feed time day . | | eat | food | | workingout'\n",
            " '@ yankeeswfan @ ken_rosenth trade sp defense-on ss ? brilliant trade .'\n",
            " '@ darklightdav tri find us , batteri die . guess found us ? yes , bastard wand ! ! ! !'\n",
            " '@ deputymartinski pleas .. need second hand embarrass desper phone']\n",
            "\n",
            "['Irony' 'Irony' 'Irony' 'Non-Irony' 'Irony' 'Non-Irony' 'Irony'\n",
            " 'Non-Irony' 'Non-Irony' 'Non-Irony' 'Irony' 'Non-Irony' 'Non-Irony'\n",
            " 'Non-Irony' 'Irony' 'Irony' 'Irony' 'Non-Irony' 'Irony' 'Irony' 'Irony']\n",
            "\n",
            "My favorite color is every color.\n",
            "favorit color everi color .\n",
            "@ClayTravisBGID One could hope. \n",
            "@ claytravisbgid one could hope .\n",
            "I love how when I'm stressed my body decides to react by causing me massive pain. \n",
            "love 'm stress bodi decid react caus massiv pain .\n",
            "Girls with British accents.. :ok_hand_sign: On point.\n",
            "girl british accent .. : ok_hand_sign : point .\n",
            "Think i almost just died, or burned my face off! #hategasbbqs\n",
            "think almost die , burn face ! hategasbbq\n",
            "#notcias #eu #europ EUA statement on European Commissions EU Investment Plan http://t.co/MXJXW5LOqz\n",
            "notcia eu europ eua statement european commiss eu invest plan http : //t.co/mxjxw5loqz\n",
            "#nowplaying EaricPatten presents \"DJ Earic Patten's Elektrik Metro House Vibes Mix Sessions on Club ...\" at http://t.co/DUSMZ0kts4\n",
            "nowplay earicpatten present `` dj earic patten 's elektrik metro hous vibe mix session club ... `` http : //t.co/dusmz0kts4\n",
            "@normwilner Unless rogue theatres start showing it, or it gets sold online?\n",
            "@ normwiln unless rogu theatr start show , get sold onlin ?\n",
            "whoever just did the hunger games whistle on the quiet floor, I applaud you on your originality and cultural relevance  #oldjoke\n",
            "whoever hunger game whistl quiet floor , applaud origin cultur relev oldjok\n",
            "Feeling like crap. And being treated horribly too. It's a great day.  #iwanttogohome\n",
            "feel like crap . treat horribl . 's great day . iwanttogohom\n",
            "@Nylons @quick13 @jamieyuccas @chadhartman it was an In progress list of \"MN of the year\"\n",
            "@ nylon @ quick13 @ jamieyucca @ chadhartman progress list `` mn year ``\n",
            "@TheFollowingFOX I get paid 4 posting stuff like this on TSU! ||YOU CAN TOO! go to http://t.co/JUmMWi0AyT||#FOLLOW #FOLLOWBACK\n",
            "@ thefollowingfox get paid 4 post stuff like tsu ! | | ! go http : //t.co/jummwi0ayt|| # follow followback\n",
            "@Abelv03 @KWAPT I just want learning from this group. Development. And to turn Green, Bass, and Thornton into assets.\n",
            "@ abelv03 @ kwapt want learn group . develop . turn green , bass , thornton asset .\n",
            "Only ones in the cinema  #putting #my #phone #on #silent @rebecca_inch http://t.co/qeid3NXXTs\n",
            "onli one cinema put phone silent @ rebecca_inch http : //t.co/qeid3nxxt\n",
            "@BBCRadMac @StuartMaconie years ago in M && S in Rochdale they had 2 left shoes on sale with a £5 reduction from full price! #bargain \n",
            "@ bbcradmac @ stuartmaconi year ago & & rochdal 2 left shoe sale 5 reduct full price ! bargain\n",
            "Montana Of 300 14 - THE BEST Versace Remix IN THE...: http://t.co/saAk4k9uzQ this dude goes off\n",
            "montana 300 14 - best versac remix ... : http : //t.co/saak4k9uzq dude goe\n",
            "I should of just made a canvas of coffee stains and be done with it. #art \n",
            "made canva coffe stain done . art\n",
            "The world is such a smiley place. :flushed_face: \n",
            "world smiley place . : flushed_fac :\n",
            "Two Broke Rednecks father/daughter riffing team who make fun of old educational films n more http://t.co/okfWzkWOmQ | http://t.co/7QG3iFJAyu\n",
            "two broke redneck father / daughter rif team make fun old educ film http : //t.co/okfwzkwomq | http : //t.co/7qg3ifjayu\n",
            "#WTF is happening to these kids??? Are you kidding me with this?? #insulting #KnowYourHistory #WhoIsPaulMcCartney http://t.co/1tjgEdxm0k\n",
            "wtf happen kid ? ? ? kid ? ? insult knowyourhistori whoispaulmccartney http : //t.co/1tjgedxm0k\n",
            "I would have made a much more convincing Bella Swan.\n",
            "would made much convinc bella swan .\n",
            "I retweeted this so Chris Graham blocked me. |:face_with_tears_of_joy: http://t.co/W9zVXuhGhx\n",
            "retweet chris graham block . | : face_with_tears_of_joy : http : //t.co/w9zvxuhghx\n",
            "Fries With That 304 #AlabamaStateMajors\n",
            "fri 304 alabamastatemajor\n",
            "@StartUpGrindBuf @magnachef If you need a #dev #environmnet #then  #a #great #pick. For #regular #web #stuff, I #do #like #it\n",
            "@ startupgrindbuf @ magnachef need dev environmnet great pick . regular web stuff , like\n",
            "I'm glad the DC Council has it's priorities intact...  #DC\n",
            "'m glad dc council 's prioriti intact ... dc\n",
            "Riding the distraction train. ||CHOO CHOO\n",
            "ride distract train . | | choo choo\n",
            "Chill #Repost #Dead  #Dominos #Haha :face_with_tears_of_joy::skull: http://t.co/SnUkkgCDB9\n",
            "chill repost dead domino haha : face_with_tears_of_joy : : skull : http : //t.co/snukkgcdb9\n",
            "Someone I work w doesn't let his kids believe in Santa because he's mythical  & at odds w Church dogma. I REALLY want 2 explain  2 him\n",
            "someon work doe n't let kid believ santa becaus 's mythic & odd church dogma . realli want 2 explain 2\n",
            "Check out my new post! MyFairDaily: 10 Things I've Been Loving Lately http://t.co/wsFo2Dlu7h #lbloggers #holidays #myfairdaily #favorites\n",
            "check new post ! myfairdaili : 10 thing love late http : //t.co/wsfo2dlu7h lblogger holiday myfairdaili favorit\n",
            "Obama whisked away to hospital, diagnosed with acid reflux  http://t.co/42WQQVHBau -Wow, that's some real news. \n",
            "obama whisk away hospit , diagnos acid reflux http : //t.co/42wqqvhbau - wow , 's real news .\n",
            "@DCsportsGrl @DragonflyJonez true n that's y we r jaded EVERY YEAR\n",
            "@ dcsportsgrl @ dragonflyjonez true 's whi jade everi year\n",
            "Another one of our support vehicles modified for Iceland service. #Bigfoot #LANDROVER http://t.co/NRXO4gFG6E\n",
            "anoth one support vehicl modifi iceland servic . bigfoot landrov http : //t.co/nrxo4gfg6\n",
            "Thanks for shutting the city down.. \n",
            "thank shut citi ..\n",
            "@flippysgardenia IKR?! don't you see? he's gonna cry from his utter joy of being with that crybaby! Such a beautiful \"couple\" \n",
            "@ flippysgardenia ikr ? ! n't see ? 's gon na cri utter joy crybabi ! beauti `` coupl ``\n",
            "Glad there's not a typhoon where we go on holiday in 4 weeks.  #fml\n",
            "glad 's typhoon go holiday 4 week . fml\n",
            "@banditelli regarding what the PSU president does\n",
            "@ banditelli regard psu presid doe\n",
            "@banditelli But still bothers me that I see now follow up report.\n",
            "@ banditelli still bother see follow report .\n",
            "well now that i've listened to all of into the woods, i'm going to listen to some FOB #nosurprisethere\n",
            "well listen wood , 'm go listen fob nosurprisether\n",
            "Hummingbirds #Are  #Experts #at #Hovering #After #All: #Background #Motion ...: If the hovering ability of the... http://t.co/E189iHBpZr\n",
            "hummingbird expert hover : background motion ... : hover abil ... http : //t.co/e189ihbpzr\n",
            "Only thing missing now is a session at the gym... Want to do every body part though and CV!  #possible\n",
            "onli thing miss session gym ... want everi bodi part though cv ! possibl\n",
            "Confusion Matrix: \n",
            "[[ 9  5]\n",
            " [10 16]]\n",
            "Recall Score:  0.625\n",
            "Precision Score:  [0.47368421 0.76190476]\n",
            "Micro-Average F1 Score:  0.625\n",
            "Accuracy:  0.625\n",
            "Evaluation Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Irony       0.47      0.64      0.55        14\n",
            "   Non-Irony       0.76      0.62      0.68        26\n",
            "\n",
            "    accuracy                           0.62        40\n",
            "   macro avg       0.62      0.63      0.61        40\n",
            "weighted avg       0.66      0.62      0.63        40\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}